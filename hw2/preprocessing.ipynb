{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/corpus.json') as f:\n",
    "    corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Текстов - 1157366, слов - 16028874'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Текстов - {}, слов - {}'.format(len(corpus), sum([len(sample) for sample in corpus]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(word):\n",
    "    \n",
    "    word_data = m.lemmatize(word)[0]\n",
    "    \n",
    "    return word_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Соберем словарь лем\n",
    "В нашем корпусе 16028874 слов. Лемматизировать весь корпус будет очень долго. Давайте лучше соберем словарь уникальных слов и будет лемматизировать только уникальные слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1157366/1157366 [03:11<00:00, 6054.75it/s] \n"
     ]
    }
   ],
   "source": [
    "tok2lemma = {}\n",
    "\n",
    "for text in tqdm(corpus):\n",
    "    for tok in text:\n",
    "        if tok not in tok2lemma:\n",
    "            tok2lemma[tok] = get_lemma(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'уезжать'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ключ - уникальное слово в нашем корпусе, значение - его лемма\n",
    "tok2lemma['уехали']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193435"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tok2lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Замена слов леммами\n",
    "Так как теперь к каждому слову из нашего корпуса мы знаем лемму, то давайте каждое слово заменим на его лемму и уберем стоп слова.\n",
    "Это работает гораздо(!) быстрее, чем если бы мы в корпусе для каждого слова каждый раз рассчитывали лемму (с помощью пайморфи), \n",
    "потому что теперь нам надо вызвать пайморфи 193435 раз вместо 16028874."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1157366/1157366 [00:42<00:00, 27265.36it/s]\n"
     ]
    }
   ],
   "source": [
    "lemmas_corpus = [[tok2lemma[tok] for tok in text if tok not in stopwords and tok]\n",
    "                 for text in tqdm(corpus)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Соберем частотный словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1157366/1157366 [00:04<00:00, 265206.85it/s]\n"
     ]
    }
   ],
   "source": [
    "freq = {}\n",
    "\n",
    "for text in tqdm(lemmas_corpus):\n",
    "    for tok in text:\n",
    "        if tok in freq:\n",
    "            freq[tok] += 1\n",
    "        else:\n",
    "            freq[tok] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df = pd.DataFrame(data={'word': list(freq.keys()), 'n_entries': list(freq.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df.sort_values(by=['n_entries'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>n_entries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>#</td>\n",
       "      <td>413016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>банк</td>\n",
       "      <td>187296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>карта</td>\n",
       "      <td>156216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>банка</td>\n",
       "      <td>146538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>кредит</td>\n",
       "      <td>86866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  n_entries\n",
       "21        #     413016\n",
       "25     банк     187296\n",
       "47    карта     156216\n",
       "3     банка     146538\n",
       "132  кредит      86866"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74882, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# уникальных слов в словаре\n",
    "freq_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>n_entries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43925</th>\n",
       "      <td>надоедливо</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43926</th>\n",
       "      <td>воспитательница</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43928</th>\n",
       "      <td>неподозревающий</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43929</th>\n",
       "      <td>признки</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74881</th>\n",
       "      <td>подпал</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  word  n_entries\n",
       "43925       надоедливо          1\n",
       "43926  воспитательница          1\n",
       "43928  неподозревающий          1\n",
       "43929          признки          1\n",
       "74881           подпал          1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = freq_df.n_entries.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Замена редких слов\n",
    "В нашем корпусе осталось много слов, которые встречаются очень редко. Давайте мы редкие слова заменим на специальный токе UNK - unknown. Так мы разительно сократим размер нашего словаря слов с незначительной потерей информации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля слов, которые мы заменим на UNK:\n",
      "Порог отсечения - 5, доля UNK - 0.74 %, слов в слове - 22271, удалили - 52611 слов\n",
      "Порог отсечения - 10, доля UNK - 1.14 %, слов в слове - 15957, удалили - 58925 слов\n",
      "Порог отсечения - 15, доля UNK - 1.46 %, слов в слове - 13099, удалили - 61783 слов\n",
      "Порог отсечения - 20, доля UNK - 1.75 %, слов в слове - 11351, удалили - 63531 слов\n",
      "Порог отсечения - 25, доля UNK - 1.99 %, слов в слове - 10201, удалили - 64681 слов\n",
      "Порог отсечения - 30, доля UNK - 2.21 %, слов в слове - 9352, удалили - 65530 слов\n",
      "Порог отсечения - 35, доля UNK - 2.42 %, слов в слове - 8667, удалили - 66215 слов\n"
     ]
    }
   ],
   "source": [
    "print('Доля слов, которые мы заменим на UNK:')\n",
    "\n",
    "for threshold in np.arange(5, 36, 5):\n",
    "    \n",
    "    sub_df = freq_df[freq_df.n_entries < threshold]\n",
    "    \n",
    "    unk_freq = sub_df['n_entries'].sum() * 100 / n_words\n",
    "    \n",
    "    print('Порог отсечения - {}, доля UNK - {:.2f} %, слов в слове - {}, удалили - {} слов'.format(\n",
    "        threshold, unk_freq, freq_df.shape[0] - sub_df.shape[0], sub_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# кажется, что оптимально, но обычно берут меньше\n",
    "threshold = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = freq_df[freq_df.n_entries >= threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(vocab.word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13099"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Мы сократили наш словарь в 5.72 раз с потерей 1.51 % всех слов'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Мы сократили наш словарь в {:.2f} раз с потерей 1.51 % всех слов'.format(freq_df.shape[0] / len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_words(word):\n",
    "    \n",
    "    if word in words:\n",
    "        return word\n",
    "    else:\n",
    "        return 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1157366/1157366 [00:07<00:00, 163092.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# заменим слово токеном UNK, если его нет в нашем новом словаре\n",
    "processed_corpus = [[get_correct_words(tok) for tok in text] for text in tqdm(lemmas_corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicate_unks(tokens):\n",
    "    \n",
    "    output_tokens = []\n",
    "    \n",
    "    for tok in tokens:\n",
    "        \n",
    "        if tok == 'UNK' and output_tokens and output_tokens[-1] == 'UNK':\n",
    "            continue\n",
    "            \n",
    "        output_tokens.append(tok)\n",
    "            \n",
    "    return output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = 'думать далее милый барышня UNK UNK тинькоф звонить неделя'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['думать',\n",
       " 'далее',\n",
       " 'милый',\n",
       " 'барышня',\n",
       " 'UNK',\n",
       " 'UNK',\n",
       " 'тинькоф',\n",
       " 'звонить',\n",
       " 'неделя']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['думать', 'далее', 'милый', 'барышня', 'UNK', 'тинькоф', 'звонить', 'неделя']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_duplicate_unks(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1157366/1157366 [00:06<00:00, 192670.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# дедублируем подряд идущие унки (оставим только один)\n",
    "processed_corpus = [drop_duplicate_unks(sample) for sample in tqdm(processed_corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Текстов с унками - 10.90 %'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_with_unk = [text for text in processed_corpus if 'UNK' in text]\n",
    "'Текстов с унками - {:.2f} %'.format(len(texts_with_unk) * 100 / len(processed_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "должный претензия ко компания ооо UNK\n",
      "свой слово UNK подтвердить принимать должный действовать клиент\n",
      "наоборот хотеть лишаться доступ деньги UNK дебетовый карта втб #\n",
      "оформлять UNK предложение кредит наличные сотрудник понадобиться # час\n",
      "звать UNK номер телефон # постоянно поступать звонок ваш банка\n"
     ]
    }
   ],
   "source": [
    "# посмотрим на тексты с унками\n",
    "for text in random.sample(texts_with_unk, k=5):\n",
    "    print(' '.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выглядит не так плохо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(processed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выберем подвыборку данных\n",
    "Чтобы быстрее выучить word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data = processed_corpus[:500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/processed_corpus_4.json', 'w') as f:\n",
    "    json.dump(sub_data, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
